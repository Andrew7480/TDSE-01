{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e0fb3dd",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf66024b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries (run this once if needed)\n",
    "%pip install numpy pandas matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c5613f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02d5ad6",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Dataset and Notation\n",
    "\n",
    "- M: stellar mass (in units of solar mass, M⊙)\n",
    "- T: effective stellar temperature (Kelvin, K)\n",
    "- L: stellar luminosity (in units of solar luminosity, L⊙\n",
    "\n",
    "M = [0.6, 0.8, 1.0, 1.2, 1.4, 1.6, 1.8, 2.0, 2.2, 2.4]\n",
    "\n",
    "L = [0.15, 0.35, 1.00, 2.30, 4.10, 7.00, 11.2, 17.5, 25.0, 35.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a26f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "M = np.array([0.6, 0.8, 1.0, 1.2, 1.4, 1.6, 1.8, 2.0, 2.2, 2.4])\n",
    "L = np.array([0.15, 0.35, 1.00, 2.30, 4.10, 7.00, 11.2, 17.5, 25.0, 35.0])\n",
    "\n",
    "m = len(M)\n",
    "l = len(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1a9ab8",
   "metadata": {},
   "source": [
    "### 2.1 DataSet Visualization: Plot M vs L. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24e9234",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(M, L)\n",
    "plt.xlabel(\"Stellar Mass (M☉)\")\n",
    "plt.ylabel(\"Luminosity (L☉)\")\n",
    "plt.title(\"Stellar Luminosity vs Mass\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5f2b3e",
   "metadata": {},
   "source": [
    "\n",
    "The plot shows a strong positive correlation between stellar mass and luminosity, that is a good sign of some plausibility.\n",
    "\n",
    "However, the relationship between both is clearly non linear: \n",
    "- luminosity increases slowly at low masses and much more rapidly at higher masses.\n",
    "\n",
    "\n",
    "This suggests that a linear regression model will only provide a rough approximation and will underpredict luminosity for high-mass stars, as the relationship is not linear at higher masses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a313a2d4",
   "metadata": {},
   "source": [
    "## 3.Model and loss\n",
    "\n",
    "The hypothesis models stellar luminosity as a linear function of mass with an explicit bias term.\n",
    "\n",
    "The mean squared error is used as the loss function, measuring the average squared difference between predicted and observed luminosities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126bccd7",
   "metadata": {},
   "source": [
    "Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a06793",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(M, w, b):\n",
    "    \"\"\"Compute predictions f_{w,b}(x) for all examples.\n",
    "    \n",
    "    Linear regression model:\n",
    "    L_hat = w * M + b\n",
    "    \n",
    "    \"\"\"\n",
    "    return M * w + b  # vectorized: matrix-vector product + scalar\n",
    "\n",
    "w_test = 0.0\n",
    "b_test = 0.0\n",
    "\n",
    "l_hat_test = predict(M, w_test, b_test)\n",
    "print(\"First 3 predictions with w = 0, b = 0:\", l_hat_test[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6b20df",
   "metadata": {},
   "source": [
    "MSE: mean squared error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb71d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## def compute_cost(x, y, w, b):\n",
    "#    m = len(x)\n",
    "#    y_hat = w * x + b\n",
    "#    return np.sum((y_hat - y) ** 2) / (2 * m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d06126f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(M, L, w, b):  #compute_cost\n",
    "    \"\"\"\n",
    "    Mean Squared Error cost function\n",
    "    \"\"\"\n",
    "    m = len(M)\n",
    "    L_hat = predict(M, w, b)\n",
    "    return (1 / (2 * m)) * np.sum((L_hat - L) ** 2) \n",
    "\n",
    "print(\"Cost with w = 0, b = 0: \", mse(M, L, w_test, b_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3485ddd6",
   "metadata": {},
   "source": [
    "## 4. Cost surface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b248f3",
   "metadata": {},
   "source": [
    "Visualizing the cost surface before applying methods like gradient descent helps us understand the behavior of the model.\n",
    "It allows us to see that there is a single global minimum, which is important to ensure that gradient descent will converge correctly.\n",
    "\n",
    "Additionally, it helps us understand the sensitivity of the parameters: how small changes in w or b affect the cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521b2a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D  # needed to register the 3D projection\n",
    "from matplotlib import cm\n",
    " \n",
    "# Choose reasonable ranges around the expected optimum\n",
    "w_vals = [float(v) for v in np.linspace(-1.0, 7.0, 60)]\n",
    "b_vals = [float(v) for v in np.linspace(-5.0, 10.0, 60)]\n",
    " \n",
    "J = np.zeros((len(w_vals), len(b_vals)))\n",
    " \n",
    "for i, w in enumerate(w_vals):\n",
    "    for j, b in enumerate(b_vals):\n",
    "        J[i, j] = mse(M, L, w, b)\n",
    "W, B = np.meshgrid(w_vals, b_vals)\n",
    " \n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "ax.plot_surface(W, B, J, cmap=cm.viridis, linewidth=0, antialiased=True)\n",
    "ax.set_xlabel(\"w\")\n",
    "ax.set_ylabel(\"b\")\n",
    "ax.set_zlabel(\"J(w,b)\")\n",
    "ax.set_title(\"Cost surface J(w,b)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12b1da0",
   "metadata": {},
   "source": [
    "The 3D plot shows how the MSE varies with slope (w) and bias (b).\n",
    "The lowest point on the surface represents the optimal parameters that best fit the data.\n",
    "Its convex shape confirms that the cost function has a single global minimum, which is why gradient descent can reliably find the optimal w and b."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2a06e4",
   "metadata": {},
   "source": [
    "## 5. Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073f878d",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial J}{\\partial w} = \\frac{1}{m} \\sum_{i=1}^{m} \\big( f_{w,b}(x^{(i)}) - y^{(i)} \\big) x^{(i)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^{m} \\big( f_{w,b}(x^{(i)}) - y^{(i)} \\big)\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "w := w - \\alpha \\frac{\\partial J}{\\partial w}, \\qquad\n",
    "b := b - \\alpha \\frac{\\partial J}{\\partial b}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a905f85",
   "metadata": {},
   "source": [
    "### 5.1 Gradient descent (non-vectorized)\n",
    "\n",
    "In this implementation, gradients are computed using explicit loops over the dataset.\n",
    "This approach closely follows the mathematical definition of gradient descent and is useful for understanding how parameter updates are performed step by step, although it is computationally inefficient for large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ece166e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_gradients(M, L, w, b):\n",
    "    m = len(M)\n",
    "    sum_dw = 0.0\n",
    "    sum_db = 0.0\n",
    "\n",
    "\n",
    "    for i in range (m):\n",
    "        error = (w * M[i] + b) - L[i]\n",
    "\n",
    "        sum_dw += error * M[i]\n",
    "        sum_db += error\n",
    "\n",
    "    sum_dw /= m\n",
    "    sum_db /= m\n",
    "    return sum_dw, sum_db\n",
    "\n",
    "dw_test, db_test = compute_gradients(M, L, w_test, b_test)\n",
    "print(\"dw =\", dw_test, \"db =\", db_test) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5855c196",
   "metadata": {},
   "source": [
    "### 5.2 Gradient descent (vectorized)\n",
    "\n",
    "This implementation computes gradients using NumPy vectorized operations instead of explicit loops.\n",
    "While mathematically equivalent to the non-vectorized version, this approach is significantly more efficient and scales better to large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606e16ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradients_vectorized(M, L, w, b):\n",
    "    m = len(M)\n",
    "    error = (w * M + b) - L\n",
    "\n",
    "    dj_dw = (1 / m) * np.sum(error * M) \n",
    "    dj_db = (1 / m) * np.sum(error)\n",
    "    return dj_dw, dj_db\n",
    "\n",
    "dj_dw_test, dj_db_test = compute_gradients(M, L, w_test, b_test)\n",
    "print(\"Gradients at w=0, b=0:\", dj_dw_test, dj_db_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c111e4e",
   "metadata": {},
   "source": [
    "### 5.3 Gradient descent (vectorized)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b65e4e6",
   "metadata": {},
   "source": [
    "In this implementation, the gradients are computed using NumPy vectorized operations without explicit loops over the dataset.\n",
    "This approach is mathematically equivalent to the non-vectorized version but is significantly more efficient and scalable.\n",
    "\n",
    "\n",
    "$$\n",
    "w := w - \\alpha \\frac{\\partial J}{\\partial w}, \\qquad\n",
    "b := b - \\alpha \\frac{\\partial J}{\\partial b}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87e3521",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_vectorized(M, L, alpha, num_iters):\n",
    "    w = 0.0\n",
    "    b = 0.0\n",
    "\n",
    "    for _ in range(num_iters):\n",
    "        dw, db = compute_gradients_vectorized(M, L, w, b)\n",
    "        w -= alpha * dw\n",
    "        b -= alpha * db\n",
    "\n",
    "    return w, b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291db3ae",
   "metadata": {},
   "source": [
    "### 5.4 Implement the Gradient Descent Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d8f54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gradient_descent(M, L, w_init, b_init, alpha, num_iterations):\n",
    "    w = w_init\n",
    "    b = b_init\n",
    "    history = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        dj_dw, dj_db = compute_gradients_vectorized(M, L, w, b)\n",
    "        w = w - alpha * dj_dw\n",
    "        b = b - alpha * dj_db\n",
    "\n",
    "        cost = mse(M, L, w, b)\n",
    "        history.append((i, cost))\n",
    "\n",
    "        if i % max(1, (num_iterations // 10)) == 0:\n",
    "            print(f\"Iteration {i:4d}: w={w:7.4f}, b={b:7.4f}, cost={cost:8.4f}\")\n",
    "\n",
    "    return w, b, history\n",
    "\n",
    "\n",
    "\n",
    "alpha = 0.01\n",
    "num_iterations = 2000\n",
    "\n",
    "w_init = 0.0\n",
    "b_init = 0.0\n",
    "\n",
    "w_learned, b_learned, history = gradient_descent(M, L, w_init, b_init, alpha, num_iterations)\n",
    "print(\"\\nLearned parameters:\")\n",
    "print(\"w =\", w_learned)\n",
    "print(\"b =\", b_learned)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c73842",
   "metadata": {},
   "source": [
    "## 6. Convergence (mandatory) Plot the Cost over Iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847d39e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(history)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Cost J(w,b)\")\n",
    "plt.title(\"Convergence of Gradient Descent: Cost vs Iterations\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9059ac71",
   "metadata": {},
   "source": [
    "The cost decreases monotonically as the number of iterations increases, indicating successful convergence.\n",
    "With the chosen learning rate, the optimization is stable and does not show oscillations or divergence.\n",
    "Most of the reduction in cost occurs during the first iterations, after which the improvement becomes gradual."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
