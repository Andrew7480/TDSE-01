{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e0fb3dd",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf66024b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Install required libraries (run this once if needed)\n",
    "%pip install numpy pandas matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c5613f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02d5ad6",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Dataset and Notation\n",
    "\n",
    "- M: stellar mass (in units of solar mass, M⊙)\n",
    "- T: effective stellar temperature (Kelvin, K)\n",
    "- L: stellar luminosity (in units of solar luminosity, L⊙\n",
    "\n",
    "M = [0.6, 0.8, 1.0, 1.2, 1.4, 1.6, 1.8, 2.0, 2.2, 2.4]\n",
    "\n",
    "L = [0.15, 0.35, 1.00, 2.30, 4.10, 7.00, 11.2, 17.5, 25.0, 35.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a26f84",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "M = np.array([0.6, 0.8, 1.0, 1.2, 1.4, 1.6, 1.8, 2.0, 2.2, 2.4])\n",
    "L = np.array([0.15, 0.35, 1.00, 2.30, 4.10, 7.00, 11.2, 17.5, 25.0, 35.0])\n",
    "\n",
    "m = len(M)\n",
    "l = len(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1a9ab8",
   "metadata": {},
   "source": [
    "### 2.1 DataSet Visualization: Plot M vs L. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24e9234",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(M, L)\n",
    "plt.xlabel(\"Stellar Mass (M☉)\")\n",
    "plt.ylabel(\"Luminosity (L☉)\")\n",
    "plt.title(\"Stellar Luminosity vs Mass\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5f2b3e",
   "metadata": {},
   "source": [
    "\n",
    "The plot shows a strong positive correlation between stellar mass and luminosity, that is a good sign of some plausibility.\n",
    "\n",
    "However, the relationship between both is clearly non linear: \n",
    "- luminosity increases slowly at low masses and much more rapidly at higher masses.\n",
    "\n",
    "\n",
    "This suggests that a linear regression model will only provide a rough approximation and will underpredict luminosity for high-mass stars, as the relationship is not linear at higher masses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a313a2d4",
   "metadata": {},
   "source": [
    "## 3.Model and loss\n",
    "\n",
    "The hypothesis models stellar luminosity as a linear function of mass with an explicit bias term.\n",
    "\n",
    "The mean squared error is used as the loss function, measuring the average squared difference between predicted and observed luminosities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126bccd7",
   "metadata": {},
   "source": [
    "Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a06793",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def predict(X, w, b):\n",
    "    \"\"\"Compute predictions f_{w,b}(x) for all examples.\n",
    "    \n",
    "    Linear regression model:\n",
    "    L_hat = w * M + b\n",
    "    \n",
    "    \"\"\"\n",
    "    return X * w + b  # vectorized: matrix-vector product + scalar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6b20df",
   "metadata": {},
   "source": [
    "MSE: mean squared error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d06126f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def mse(M, L, w, b):\n",
    "    \"\"\"\n",
    "    Mean Squared Error cost function\n",
    "    \"\"\"\n",
    "    m = len(M)\n",
    "    L_hat = predict(M, w, b)\n",
    "    return (1 / (2 * m)) * np.sum((L_hat - L) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3485ddd6",
   "metadata": {},
   "source": [
    "## 4 Cost surface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b248f3",
   "metadata": {},
   "source": [
    "Visualizing the cost surface before applying methods like gradient descent helps us understand the behavior of the model.\n",
    "It allows us to see that there is a single global minimum, which is important to ensure that gradient descent will converge correctly.\n",
    "\n",
    "Additionally, it helps us understand the sensitivity of the parameters: how small changes in w or b affect the cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521b2a43",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D  # needed to register the 3D projection\n",
    "from matplotlib import cm\n",
    " \n",
    "# Choose reasonable ranges around the expected optimum\n",
    "w_vals = [float(v) for v in np.linspace(-1.0, 7.0, 60)]\n",
    "b_vals = [float(v) for v in np.linspace(-5.0, 10.0, 60)]\n",
    " \n",
    "J = np.zeros((len(w_vals), len(b_vals)))\n",
    " \n",
    "for i, w in enumerate(w_vals):\n",
    "    for j, b in enumerate(b_vals):\n",
    "        J[i, j] = mse(M, L, w, b)\n",
    "W, B = np.meshgrid(w_vals, b_vals)\n",
    " \n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "ax.plot_surface(W, B, J, cmap=cm.viridis, linewidth=0, antialiased=True)\n",
    "ax.set_xlabel(\"w\")\n",
    "ax.set_ylabel(\"b\")\n",
    "ax.set_zlabel(\"J(w,b)\")\n",
    "ax.set_title(\"Cost surface J(w,b)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12b1da0",
   "metadata": {},
   "source": [
    "The 3D plot shows how the MSE varies with slope (w) and bias (b).\n",
    "The lowest point on the surface represents the optimal parameters that best fit the data.\n",
    "Its convex shape confirms that the cost function has a single global minimum, which is why gradient descent can reliably find the optimal w and b."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
